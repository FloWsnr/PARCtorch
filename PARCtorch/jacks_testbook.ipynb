{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0da705-b607-4892-80b0-d2a8692bc1c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing navier stokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d34181fc-3a3d-4f91-892d-396fc436059e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PARCtorch.PARCv2 import PARCv2\n",
    "from PARCtorch.differentiator.differentiator import Differentiator\n",
    "from PARCtorch.differentiator.finitedifference import FiniteDifference\n",
    "from PARCtorch.integrator.integrator import Integrator\n",
    "from PARCtorch.integrator.heun import Heun\n",
    "from PARCtorch.utilities.unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80af185f-dde4-4452-9e1c-95204a04e893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6502de5-b7e0-43bd-aa70-c31d721da83b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Navier Stokes: p, re, u, v\n",
    "# Adv: u, v\n",
    "# Dif: u, v\n",
    "# Poi: p\n",
    "n_fe_features = 128\n",
    "unet_ns = UNet([64, 64*2, 64*4, 64*8, 64*16], 4, n_fe_features, up_block_use_concat=[False, True, False, True], skip_connection_indices=[2, 0])\n",
    "right_diff = FiniteDifference(padding_mode=\"replicate\").cuda()\n",
    "heun_int = Heun().cuda()\n",
    "diff_ns = Differentiator(2,                 # 2 state variables: p, re. We always assume 2 velocity being the last 2 channels \n",
    "                         n_fe_features,     # Number of features returned by the feature extraction network: 64\n",
    "                         [2, 3],            # Channel indices to calculate advection: u and v\n",
    "                         [2, 3],            # Channel indices to calculate diffusion: u and v\n",
    "                         unet_ns,           # Feature extraction network: unet_burgers\n",
    "                         \"constant\",        # Padding mode: constant padding of zero\n",
    "                         right_diff        # Finite difference method: replication of image_gradients\n",
    "                         \n",
    "                        ).cuda()\n",
    "ns_int = Integrator(True,                     # Clip input data between 0 and 1\n",
    "                    [(0, 2, 3, 1)],           # Poisson: calculate Poisson on indices 2 (u) and 3 (v), combine it with all the input and output to index 0 (p) \n",
    "                    heun_int,                 # Heun integration\n",
    "                    [None, None, None, None], # No data driven integrator\n",
    "                    \"constant\",               # Padding mode: constant padding of zero\n",
    "                    right_diff,                # Finite difference method: replication of image_gradients\n",
    "                   ).cuda()\n",
    "criterion = torch.nn.L1Loss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b63c6a8a-5590-451d-bb9b-7c1aebefe6f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "differentiator\n",
      "differentiator.list_adv\n",
      "differentiator.list_adv.2\n",
      "differentiator.list_adv.2.cdiff\n",
      "differentiator.list_adv.3\n",
      "differentiator.list_dif\n",
      "differentiator.list_dif.2\n",
      "differentiator.list_dif.3\n",
      "differentiator.list_mar\n",
      "differentiator.list_mar.2\n",
      "differentiator.list_mar.2.spade\n",
      "differentiator.list_mar.2.spade.leaky_relu\n",
      "differentiator.list_mar.2.spade.spade1\n",
      "differentiator.list_mar.2.spade.spade1.initial_conv\n",
      "differentiator.list_mar.2.spade.spade1.initial_conv.0\n",
      "differentiator.list_mar.2.spade.spade1.initial_conv.1\n",
      "differentiator.list_mar.2.spade.spade1.gamma_conv\n",
      "differentiator.list_mar.2.spade.spade1.beta_conv\n",
      "differentiator.list_mar.2.spade.conv1\n",
      "differentiator.list_mar.2.spade.spade2\n",
      "differentiator.list_mar.2.spade.spade2.initial_conv\n",
      "differentiator.list_mar.2.spade.spade2.initial_conv.0\n",
      "differentiator.list_mar.2.spade.spade2.initial_conv.1\n",
      "differentiator.list_mar.2.spade.spade2.gamma_conv\n",
      "differentiator.list_mar.2.spade.spade2.beta_conv\n",
      "differentiator.list_mar.2.spade.conv2\n",
      "differentiator.list_mar.2.spade.spade_skip\n",
      "differentiator.list_mar.2.spade.spade_skip.initial_conv\n",
      "differentiator.list_mar.2.spade.spade_skip.initial_conv.0\n",
      "differentiator.list_mar.2.spade.spade_skip.initial_conv.1\n",
      "differentiator.list_mar.2.spade.spade_skip.gamma_conv\n",
      "differentiator.list_mar.2.spade.spade_skip.beta_conv\n",
      "differentiator.list_mar.2.spade.conv_skip\n",
      "differentiator.list_mar.2.resnet\n",
      "differentiator.list_mar.2.resnet.conv1\n",
      "differentiator.list_mar.2.resnet.conv1.0\n",
      "differentiator.list_mar.2.resnet.conv1.1\n",
      "differentiator.list_mar.2.resnet.conv2\n",
      "differentiator.list_mar.2.resnet.conv2.0\n",
      "differentiator.list_mar.2.resnet.conv2.1\n",
      "differentiator.list_mar.2.resnet.path\n",
      "differentiator.list_mar.2.resnet.path.0\n",
      "differentiator.list_mar.2.resnet.path.0.conv1\n",
      "differentiator.list_mar.2.resnet.path.0.conv1.0\n",
      "differentiator.list_mar.2.resnet.path.0.conv1.1\n",
      "differentiator.list_mar.2.resnet.path.0.conv2\n",
      "differentiator.list_mar.2.resnet.path.0.conv2.0\n",
      "differentiator.list_mar.2.resnet.path.0.conv2.1\n",
      "differentiator.list_mar.2.resnet.path.0.skip_conv\n",
      "differentiator.list_mar.2.resnet.path.0.relu2\n",
      "differentiator.list_mar.2.resnet.path.1\n",
      "differentiator.list_mar.2.resnet.path.1.conv1\n",
      "differentiator.list_mar.2.resnet.path.1.conv1.0\n",
      "differentiator.list_mar.2.resnet.path.1.conv1.1\n",
      "differentiator.list_mar.2.resnet.path.1.conv2\n",
      "differentiator.list_mar.2.resnet.path.1.conv2.0\n",
      "differentiator.list_mar.2.resnet.path.1.conv2.1\n",
      "differentiator.list_mar.2.resnet.path.1.skip_conv\n",
      "differentiator.list_mar.2.resnet.path.1.relu2\n",
      "differentiator.list_mar.2.conv_out\n",
      "differentiator.feature_extraction\n",
      "differentiator.feature_extraction.doubleConv\n",
      "differentiator.feature_extraction.doubleConv.0\n",
      "differentiator.feature_extraction.doubleConv.1\n",
      "differentiator.feature_extraction.doubleConv.2\n",
      "differentiator.feature_extraction.doubleConv.3\n",
      "differentiator.feature_extraction.downBlocks\n",
      "differentiator.feature_extraction.downBlocks.0\n",
      "differentiator.feature_extraction.downBlocks.0.doubleConv\n",
      "differentiator.feature_extraction.downBlocks.0.doubleConv.0\n",
      "differentiator.feature_extraction.downBlocks.0.doubleConv.1\n",
      "differentiator.feature_extraction.downBlocks.0.doubleConv.2\n",
      "differentiator.feature_extraction.downBlocks.0.doubleConv.3\n",
      "differentiator.feature_extraction.downBlocks.0.pool\n",
      "differentiator.feature_extraction.downBlocks.1\n",
      "differentiator.feature_extraction.downBlocks.1.doubleConv\n",
      "differentiator.feature_extraction.downBlocks.1.doubleConv.0\n",
      "differentiator.feature_extraction.downBlocks.1.doubleConv.1\n",
      "differentiator.feature_extraction.downBlocks.1.doubleConv.2\n",
      "differentiator.feature_extraction.downBlocks.1.doubleConv.3\n",
      "differentiator.feature_extraction.downBlocks.1.pool\n",
      "differentiator.feature_extraction.downBlocks.2\n",
      "differentiator.feature_extraction.downBlocks.2.doubleConv\n",
      "differentiator.feature_extraction.downBlocks.2.doubleConv.0\n",
      "differentiator.feature_extraction.downBlocks.2.doubleConv.1\n",
      "differentiator.feature_extraction.downBlocks.2.doubleConv.2\n",
      "differentiator.feature_extraction.downBlocks.2.doubleConv.3\n",
      "differentiator.feature_extraction.downBlocks.2.pool\n",
      "differentiator.feature_extraction.downBlocks.3\n",
      "differentiator.feature_extraction.downBlocks.3.doubleConv\n",
      "differentiator.feature_extraction.downBlocks.3.doubleConv.0\n",
      "differentiator.feature_extraction.downBlocks.3.doubleConv.1\n",
      "differentiator.feature_extraction.downBlocks.3.doubleConv.2\n",
      "differentiator.feature_extraction.downBlocks.3.doubleConv.3\n",
      "differentiator.feature_extraction.downBlocks.3.pool\n",
      "differentiator.feature_extraction.upBlocks\n",
      "differentiator.feature_extraction.upBlocks.0\n",
      "differentiator.feature_extraction.upBlocks.0.upConv\n",
      "differentiator.feature_extraction.upBlocks.0.doubleConv\n",
      "differentiator.feature_extraction.upBlocks.0.doubleConv.0\n",
      "differentiator.feature_extraction.upBlocks.0.doubleConv.1\n",
      "differentiator.feature_extraction.upBlocks.0.doubleConv.2\n",
      "differentiator.feature_extraction.upBlocks.0.doubleConv.3\n",
      "differentiator.feature_extraction.upBlocks.1\n",
      "differentiator.feature_extraction.upBlocks.1.upConv\n",
      "differentiator.feature_extraction.upBlocks.1.doubleConv\n",
      "differentiator.feature_extraction.upBlocks.1.doubleConv.0\n",
      "differentiator.feature_extraction.upBlocks.1.doubleConv.1\n",
      "differentiator.feature_extraction.upBlocks.1.doubleConv.2\n",
      "differentiator.feature_extraction.upBlocks.1.doubleConv.3\n",
      "differentiator.feature_extraction.upBlocks.2\n",
      "differentiator.feature_extraction.upBlocks.2.upConv\n",
      "differentiator.feature_extraction.upBlocks.2.doubleConv\n",
      "differentiator.feature_extraction.upBlocks.2.doubleConv.0\n",
      "differentiator.feature_extraction.upBlocks.2.doubleConv.1\n",
      "differentiator.feature_extraction.upBlocks.2.doubleConv.2\n",
      "differentiator.feature_extraction.upBlocks.2.doubleConv.3\n",
      "differentiator.feature_extraction.upBlocks.3\n",
      "differentiator.feature_extraction.upBlocks.3.upConv\n",
      "differentiator.feature_extraction.upBlocks.3.doubleConv\n",
      "differentiator.feature_extraction.upBlocks.3.doubleConv.0\n",
      "differentiator.feature_extraction.upBlocks.3.doubleConv.1\n",
      "differentiator.feature_extraction.upBlocks.3.doubleConv.2\n",
      "differentiator.feature_extraction.upBlocks.3.doubleConv.3\n",
      "differentiator.feature_extraction.finalConv\n",
      "differentiator.feature_extraction.finalConv.0\n",
      "differentiator.feature_extraction.finalConv.1\n",
      "differentiator.feature_extraction.finalConv.2\n",
      "differentiator.feature_extraction.finalConv.3\n",
      "integrator\n",
      "integrator.numerical_integrator\n",
      "integrator.list_datadriven_integrator\n",
      "integrator.list_poi\n",
      "integrator.list_poi.0\n",
      "integrator.list_poi.0.poisson\n",
      "integrator.list_poi.0.conv\n",
      "integrator.list_poi.0.conv.0\n",
      "integrator.list_poi.0.conv.1\n",
      "integrator.list_poi.0.conv.2\n",
      "integrator.list_poi.0.conv.3\n",
      "integrator.list_poi.0.conv.4\n",
      "integrator.list_poi.0.conv.4.conv1\n",
      "integrator.list_poi.0.conv.4.conv1.0\n",
      "integrator.list_poi.0.conv.4.conv1.1\n",
      "integrator.list_poi.0.conv.4.conv2\n",
      "integrator.list_poi.0.conv.4.conv2.0\n",
      "integrator.list_poi.0.conv.4.conv2.1\n",
      "integrator.list_poi.0.conv.4.path\n",
      "integrator.list_poi.0.conv.4.path.0\n",
      "integrator.list_poi.0.conv.4.path.0.conv1\n",
      "integrator.list_poi.0.conv.4.path.0.conv1.0\n",
      "integrator.list_poi.0.conv.4.path.0.conv1.1\n",
      "integrator.list_poi.0.conv.4.path.0.conv2\n",
      "integrator.list_poi.0.conv.4.path.0.conv2.0\n",
      "integrator.list_poi.0.conv.4.path.0.conv2.1\n",
      "integrator.list_poi.0.conv.4.path.0.skip_conv\n",
      "integrator.list_poi.0.conv.4.path.0.relu2\n",
      "integrator.list_poi.0.conv.4.path.1\n",
      "integrator.list_poi.0.conv.4.path.1.conv1\n",
      "integrator.list_poi.0.conv.4.path.1.conv1.0\n",
      "integrator.list_poi.0.conv.4.path.1.conv1.1\n",
      "integrator.list_poi.0.conv.4.path.1.conv2\n",
      "integrator.list_poi.0.conv.4.path.1.conv2.0\n",
      "integrator.list_poi.0.conv.4.path.1.conv2.1\n",
      "integrator.list_poi.0.conv.4.path.1.skip_conv\n",
      "integrator.list_poi.0.conv.4.path.1.relu2\n",
      "integrator.list_poi.0.conv.5\n",
      "loss\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44e3ec8d-ca07-49aa-966b-c0122ee14343",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 14:07:46,550 [INFO] Model weights loaded from /sfs/qumulo/qhome/jtb3sud/hold/navier_stokes.pt\n",
      "2024-10-07 14:07:46,553 [INFO] Starting data validation...\n",
      "2024-10-07 14:07:46,555 [INFO] Validating .npy files...\n",
      "Validating files: 100%|██████████| 8/8 [00:00<00:00, 540.29it/s]\n",
      "2024-10-07 14:07:46,575 [INFO] Data validation completed successfully.\n",
      "2024-10-07 14:07:46,577 [INFO] Preparing dataset samples...\n",
      "Listing samples: 100%|██████████| 8/8 [00:00<00:00, 714.70it/s]\n",
      "2024-10-07 14:07:46,593 [INFO] Total samples in dataset: 304\n",
      "2024-10-07 14:07:46,596 [INFO] Starting model evaluation on test data...\n",
      "2024-10-07 14:07:46,712 [INFO] Batch 1/76, Loss: 0.2177707850933075\n",
      "2024-10-07 14:07:46,764 [INFO] Batch 2/76, Loss: 0.21459530293941498\n",
      "2024-10-07 14:07:46,814 [INFO] Batch 3/76, Loss: 0.21219569444656372\n",
      "2024-10-07 14:07:46,853 [INFO] Batch 4/76, Loss: 0.21145108342170715\n",
      "2024-10-07 14:07:46,891 [INFO] Batch 5/76, Loss: 0.213944673538208\n",
      "2024-10-07 14:07:46,937 [INFO] Batch 6/76, Loss: 0.21852551400661469\n",
      "2024-10-07 14:07:46,983 [INFO] Batch 7/76, Loss: 0.21973511576652527\n",
      "2024-10-07 14:07:47,024 [INFO] Batch 8/76, Loss: 0.21944265067577362\n",
      "2024-10-07 14:07:47,086 [INFO] Batch 9/76, Loss: 0.2192264050245285\n",
      "2024-10-07 14:07:47,124 [INFO] Batch 10/76, Loss: 0.1762697398662567\n",
      "2024-10-07 14:07:47,162 [INFO] Batch 11/76, Loss: 0.12597514688968658\n",
      "2024-10-07 14:07:47,208 [INFO] Batch 12/76, Loss: 0.12334604561328888\n",
      "2024-10-07 14:07:47,252 [INFO] Batch 13/76, Loss: 0.1250952184200287\n",
      "2024-10-07 14:07:47,297 [INFO] Batch 14/76, Loss: 0.1306290179491043\n",
      "2024-10-07 14:07:47,343 [INFO] Batch 15/76, Loss: 0.13213428854942322\n",
      "2024-10-07 14:07:47,382 [INFO] Batch 16/76, Loss: 0.13112220168113708\n",
      "2024-10-07 14:07:47,420 [INFO] Batch 17/76, Loss: 0.13268545269966125\n",
      "2024-10-07 14:07:47,459 [INFO] Batch 18/76, Loss: 0.13353070616722107\n",
      "2024-10-07 14:07:47,505 [INFO] Batch 19/76, Loss: 0.1343565285205841\n",
      "2024-10-07 14:07:47,543 [INFO] Batch 20/76, Loss: 0.21444080770015717\n",
      "2024-10-07 14:07:47,589 [INFO] Batch 21/76, Loss: 0.21118399500846863\n",
      "2024-10-07 14:07:47,627 [INFO] Batch 22/76, Loss: 0.20893466472625732\n",
      "2024-10-07 14:07:47,673 [INFO] Batch 23/76, Loss: 0.2092038094997406\n",
      "2024-10-07 14:07:47,712 [INFO] Batch 24/76, Loss: 0.21315807104110718\n",
      "2024-10-07 14:07:47,750 [INFO] Batch 25/76, Loss: 0.21690112352371216\n",
      "2024-10-07 14:07:47,789 [INFO] Batch 26/76, Loss: 0.2156619131565094\n",
      "2024-10-07 14:07:47,833 [INFO] Batch 27/76, Loss: 0.21524469554424286\n",
      "2024-10-07 14:07:47,874 [INFO] Batch 28/76, Loss: 0.21535858511924744\n",
      "2024-10-07 14:07:47,912 [INFO] Batch 29/76, Loss: 0.21439431607723236\n",
      "2024-10-07 14:07:47,958 [INFO] Batch 30/76, Loss: 0.20740428566932678\n",
      "2024-10-07 14:07:47,997 [INFO] Batch 31/76, Loss: 0.20489200949668884\n",
      "2024-10-07 14:07:48,035 [INFO] Batch 32/76, Loss: 0.20385757088661194\n",
      "2024-10-07 14:07:48,090 [INFO] Batch 33/76, Loss: 0.20697003602981567\n",
      "2024-10-07 14:07:48,145 [INFO] Batch 34/76, Loss: 0.210145503282547\n",
      "2024-10-07 14:07:48,183 [INFO] Batch 35/76, Loss: 0.21000121533870697\n",
      "2024-10-07 14:07:48,221 [INFO] Batch 36/76, Loss: 0.2098073959350586\n",
      "2024-10-07 14:07:48,260 [INFO] Batch 37/76, Loss: 0.2098548412322998\n",
      "2024-10-07 14:07:48,300 [INFO] Batch 38/76, Loss: 0.20975014567375183\n",
      "2024-10-07 14:07:48,342 [INFO] Batch 39/76, Loss: 0.2045654058456421\n",
      "2024-10-07 14:07:48,387 [INFO] Batch 40/76, Loss: 0.20098194479942322\n",
      "2024-10-07 14:07:48,425 [INFO] Batch 41/76, Loss: 0.19909775257110596\n",
      "2024-10-07 14:07:48,464 [INFO] Batch 42/76, Loss: 0.20088981091976166\n",
      "2024-10-07 14:07:48,510 [INFO] Batch 43/76, Loss: 0.20427559316158295\n",
      "2024-10-07 14:07:48,556 [INFO] Batch 44/76, Loss: 0.2058136761188507\n",
      "2024-10-07 14:07:48,603 [INFO] Batch 45/76, Loss: 0.2056545466184616\n",
      "2024-10-07 14:07:48,648 [INFO] Batch 46/76, Loss: 0.20459794998168945\n",
      "2024-10-07 14:07:48,686 [INFO] Batch 47/76, Loss: 0.20474249124526978\n",
      "2024-10-07 14:07:48,731 [INFO] Batch 48/76, Loss: 0.2048129290342331\n",
      "2024-10-07 14:07:48,769 [INFO] Batch 49/76, Loss: 0.19905048608779907\n",
      "2024-10-07 14:07:48,807 [INFO] Batch 50/76, Loss: 0.1964477151632309\n",
      "2024-10-07 14:07:48,845 [INFO] Batch 51/76, Loss: 0.19666984677314758\n",
      "2024-10-07 14:07:48,884 [INFO] Batch 52/76, Loss: 0.19928187131881714\n",
      "2024-10-07 14:07:48,922 [INFO] Batch 53/76, Loss: 0.20259380340576172\n",
      "2024-10-07 14:07:48,968 [INFO] Batch 54/76, Loss: 0.20148615539073944\n",
      "2024-10-07 14:07:49,007 [INFO] Batch 55/76, Loss: 0.20207464694976807\n",
      "2024-10-07 14:07:49,045 [INFO] Batch 56/76, Loss: 0.20113331079483032\n",
      "2024-10-07 14:07:49,103 [INFO] Batch 57/76, Loss: 0.20177260041236877\n",
      "2024-10-07 14:07:49,154 [INFO] Batch 58/76, Loss: 0.044517915695905685\n",
      "2024-10-07 14:07:49,192 [INFO] Batch 59/76, Loss: 0.05119311437010765\n",
      "2024-10-07 14:07:49,230 [INFO] Batch 60/76, Loss: 0.06462930142879486\n",
      "2024-10-07 14:07:49,269 [INFO] Batch 61/76, Loss: 0.05973348766565323\n",
      "2024-10-07 14:07:49,307 [INFO] Batch 62/76, Loss: 0.06391841173171997\n",
      "2024-10-07 14:07:49,345 [INFO] Batch 63/76, Loss: 0.062251269817352295\n",
      "2024-10-07 14:07:49,384 [INFO] Batch 64/76, Loss: 0.06345893442630768\n",
      "2024-10-07 14:07:49,424 [INFO] Batch 65/76, Loss: 0.061706289649009705\n",
      "2024-10-07 14:07:49,464 [INFO] Batch 66/76, Loss: 0.06394889205694199\n",
      "2024-10-07 14:07:49,507 [INFO] Batch 67/76, Loss: 0.13271674513816833\n",
      "2024-10-07 14:07:49,553 [INFO] Batch 68/76, Loss: 0.19406795501708984\n",
      "2024-10-07 14:07:49,594 [INFO] Batch 69/76, Loss: 0.19144903123378754\n",
      "2024-10-07 14:07:49,641 [INFO] Batch 70/76, Loss: 0.19259461760520935\n",
      "2024-10-07 14:07:49,679 [INFO] Batch 71/76, Loss: 0.19564789533615112\n",
      "2024-10-07 14:07:49,718 [INFO] Batch 72/76, Loss: 0.19802618026733398\n",
      "2024-10-07 14:07:49,756 [INFO] Batch 73/76, Loss: 0.1981666535139084\n",
      "2024-10-07 14:07:49,794 [INFO] Batch 74/76, Loss: 0.1972360908985138\n",
      "2024-10-07 14:07:49,832 [INFO] Batch 75/76, Loss: 0.19642499089241028\n",
      "2024-10-07 14:07:49,870 [INFO] Batch 76/76, Loss: 0.1964123547077179\n",
      "2024-10-07 14:07:49,909 [INFO] Model evaluation completed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import logging\n",
    "from data.dataset import GenericPhysicsDataset, custom_collate_fn, visualize_channels\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "\n",
    "# Model and paths\n",
    "model = PARCv2(diff_ns, ns_int, criterion).cuda()\n",
    "weights_path = '/sfs/qumulo/qhome/jtb3sud/hold/navier_stokes.pt'  # Replace with your model weights path\n",
    "\n",
    "# Load model weights\n",
    "if os.path.exists(weights_path):\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "    logging.info(f\"Model weights loaded from {weights_path}\")\n",
    "else:\n",
    "    logging.error(f\"Model weights not found at {weights_path}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Path configurations for datasets\n",
    "data_dir_test = '/project/vil_baek/data/physics/PARCTorch/NavierStokes/test'    # Replace with your actual test directory path\n",
    "min_max_path = '/sfs/qumulo/qhome/jtb3sud/PARCtorch/PARCtorch/data/min_max.json'\n",
    "\n",
    "future_steps = 1\n",
    "batch_size = 4\n",
    "\n",
    "# Initialize the dataset and loader for testing\n",
    "test_dataset = GenericPhysicsDataset(\n",
    "    data_dirs=[data_dir_test],\n",
    "    future_steps=future_steps,\n",
    "    min_max_path=min_max_path\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "# Testing loop\n",
    "# Testing loop\n",
    "logging.info(\"Starting model evaluation on test data...\")\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        # Assuming batch contains ic, t0, t1, and ground truth (gt)\n",
    "        ic, t0, t1, gt = batch  # Unpack the batch tuple\n",
    "        \n",
    "        # Move the data to the GPU\n",
    "        ic = ic.cuda()\n",
    "        t0 = t0.cuda()\n",
    "        t1 = t1.cuda()\n",
    "        gt = gt.cuda()\n",
    "\n",
    "        # Model prediction\n",
    "        predictions = model(ic, t0, t1)\n",
    "\n",
    "        # Optionally calculate and log the loss or other metrics\n",
    "        loss = criterion(predictions[:, :, 1:, :, :], gt[:, :, 1:, :, :])\n",
    "        logging.info(f\"Batch {batch_idx+1}/{len(test_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "        \n",
    "\n",
    "logging.info(\"Model evaluation completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
